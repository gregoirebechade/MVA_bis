\providecommand\numberofexercises{}
\XSIM{solution-body}{exercise-1=={\par \begin {equation} \lambda _{\max } = \dots \end {equation} \par }||exercise-2=={\par \begin {equation} \lambda _{\max } = \dots \end {equation} \par }||exercise-3=={\par $\gamma (\tau ) = \mathbb {E}(X_n X_{n+\tau })$\\ We have that $X_n \sim \mathcal {N}(0, \sigma ^2)$. Therefore, if $\tau = 0$, we have directly that $\gamma (0) = \mathbb {E}(X_n^2) = \sigma ^2$. If $\tau \neq 0$, we have that $\gamma (\tau ) = \mathbb {E}(X_n X_{n+\tau }) = \mathbb {E}(X_n) \mathbb {E}(X_{n+\tau }) = 0$, as $X_n$ is a white noise. \par Finally, $\gamma (\tau ) = \mathbb {1}_{ \{\tau =0\}} \sigma ^2$.\\ \par \begin {align} S(f) &= \sum _{\tau =-\infty }^{+\infty }\gamma (\tau )e^{-2\pi f\tau /f_s} \\ &= \gamma (0) e^{-2\pi f \times 0/f_s} \\ &= \sigma ^2\\ \end {align} \par }||exercise-4=={\par \begin {align} \mathbb {E}(\hat {\gamma } (\tau )) &= \mathbb {E}(\frac {1}{N} \sum _{n=0}^{N-\tau -1} X_n X_{n+\tau })\\ &= \frac {1}{N} \sum _{n=0}^{N-\tau -1} \mathbb {E}(X_n X_{n+\tau })\\ &= \frac {N-\tau }{N} \gamma (\tau )\\ & \neq \gamma (\tau ) \end {align} \par This estimator is therefore \textbf {biased}. However, $\lim _{N \to \infty } \mathbb {E}(\hat {\gamma }(\tau )) = \gamma (\tau )$, so it is \textbf {asymptotically unbiased}. A simple way to de-bias this estimator is to multiply it by $\frac {N}{N-\tau }$.}||exercise-5=={\par }||exercise-6=={\par We observe that the autocovariance function converges towards the expected value ($\mathbb {1}_{\{0\}}$). The fact that the standard deviation decreases with $N$ is expected, as a bigger $N$ means more samples and therefore a more accurate estimation of the autocovariance function. However, the fact that the standard deviation decreases when $\tau $ increases is surprising: when $\tau $ increases, the number of samples in the set from which the standard deviation is computed decreases, which should lead to a bigger variance. This decrease comes from the fact that the $\hat {\gamma }$ is a sum of $N-\tau -1$ divided by $N$, leading to smaller values when $\tau $ gets bigger. We would probably observe a different behaviour (with a standard deviation increasing with $\tau $) with the unbiaised estimator.\\ \par Increasing the length of the samples does not have any affect on the periodogram.}||exercise-7=={\par \par $Var(\hat {\gamma }(\tau )) = \mathbb {E}(\hat {\gamma }(\tau )^2) - \mathbb {E}(\hat {\gamma }(\tau ))^2$\\[1cm] $\mathbb {E}((\sum _{n=0}^{N-\tau -1} X_n X_{n+\tau })^2) =\mathbb {E} (\sum _{n=0}^{N-\tau -1} X_n X_{n+\tau } \sum _{m=0}^{N-\tau -1} X_m X_{m+\tau }) \\ = \sum _{n=0}^{N-\tau -1} \sum _{m=0}^{N-\tau -1} \mathbb {E}(X_n X_{n+\tau } X_m X_{m+\tau })$\\ As the $X_n$ are jointly Gaussian variables, we have :\\ \par $\mathbb {E}((\sum _{n=0}^{N-\tau -1} X_n X_{n+\tau })^2) = \sum _{n=0}^{N-\tau -1} \sum _{m=0}^{N-\tau -1} (\mathbb {E}(X_n X_{n+\tau }) \mathbb {E}(X_m X_{m+\tau }) + \mathbb {E}(X_n X_m) \mathbb {E}(X_{n+\tau } X_{m+\tau }) + \mathbb {E}(X_n X_{m+\tau }) \mathbb {E}(X_{n+\tau } X_m))$\\ $=\sum _{n=0}^{N-\tau -1} \sum _{m=0}^{N-\tau -1} \gamma (n-m)^2 + \gamma (\tau )^2 + \gamma (\tau + m - n ) \gamma (\tau + n - m ) $\\ \par However, we have that $\mathbb {E}(\hat {\gamma })^2 = (\frac {N- \tau }{N} \gamma (\tau ))^2$, which leads to \\ \par $Var(\hat {\gamma }(\tau )) = \frac {1}{N^2} \times \sum _{n=0}^{N-\tau -1} \sum _{m=0}^{N-\tau -1} \gamma (n-m)^2 + \gamma (\tau + m - n ) \gamma (\tau + n - m ) $\\. \par Therefore, it is relevant to \textbf {sum on the difference between $n$ and $m$}. $n-m$ takes values in $[-(N-\tau -1), N-\tau -1]$, and a value $n'$ in $[-(N-\tau -1), N-\tau -1]$ is taken $N - \tau - |n'|$ times in the double sum. \par We finally get the result expected: \\ \par $Var(\hat {\gamma }(\tau )) = \frac {1}{N} \times \sum _{n=-(N-\tau -1)}^{n=N-\tau -1} \left ( \frac {N - \tau - |n|}{N}\right ) \left (\gamma ^2(n) + \gamma (n-\tau )\gamma (n+\tau )\right ).$ \\ \par \par To prove that $\hat {\gamma }$ is consistent, we need to show that \textbf {$Var(\hat {\gamma }(\tau )) \xrightarrow [n \to \infty ]{}$0}, which comes directly from the absolute and square sommabilities of the covariances. Let $\epsilon \in \mathbb {R}^*_+$. \par $\mathbb {P}(|\hat {\gamma }(\tau ) - \gamma (\tau )| > \epsilon ) \leq \frac {Var(\hat {\gamma }(\tau ))}{\epsilon ^2}\xrightarrow [ n \to \infty ]{} $0. (With \textbf {BienaymÃ©-Tchebychev inequality}). \par \par \par \par \par }||exercise-8=={\par }||exercise-9=={\par \begin {figure} \centering \begin {minipage}[t]{0.3\textwidth } \centerline {\includegraphics [width=\textwidth ]{example-image-golden}} \centerline {Periodogram ($N=200$)} \end {minipage} \begin {minipage}[t]{0.3\textwidth } \centerline {\includegraphics [width=\textwidth ]{example-image-golden}} \centerline {Periodogram ($N=500$)} \end {minipage} \begin {minipage}[t]{0.3\textwidth } \centerline {\includegraphics [width=\textwidth ]{example-image-golden}} \centerline {Periodogram ($N=1000$)} \end {minipage} \vskip 1em \caption {Barlett's periodograms of a Gaussian white noise (see Question~\ref {q:barlett}).} \label {fig:barlett} \end {figure} \par }||exercise-10=={\par \par \par We have splitted the train set in 5 patches and have trained a KNN on 4 of them and tested on the last one. We have then computed the F1-score for each of these 5 situations and averaged the results for each number of neighbors. The optimal number of neighbourghs is \textbf {k=3}, with a \textbf {F1-score of 0.58}. We have then trained the model on the whole train set and tested it on the test set, obtaining a \textbf {F1-score of 0.57.} The huge difference between those two numbers can be explained by the fact that the train set is balanced, while the test set has a proportion of 0.82 unhealthy steps. \par }||exercise-11=={\begin {figure} \centering \begin {minipage}[t]{\textwidth } \centerline {\includegraphics [width=0.6\textwidth ]{Healthy step badly classified.png}} \centerline {Badly classified healthy step} \end {minipage} \vskip 1em \begin {minipage}[t]{\textwidth } \centerline {\includegraphics [width=0.6\textwidth ]{Non healthy step badly classified.png}} \centerline {Badly classified non-healthy step} \end {minipage} \vskip 1em \caption {Examples of badly classified steps (see Question~\ref {q:class-errors}).} \label {fig:class-errors} \end {figure}}}
\XSIM{exercise-body}{exercise-1=={Consider the following Lasso regression: \begin {equation}\label {eq:lasso} \min _{\beta \in \RR ^p} \frac {1}{2}\norm {y-X\beta }^2_2 \quad + \quad \lambda \norm {\beta }_1 \end {equation} where $y\in \RR ^n$ is the response vector, $X\in \RR ^{n\times p}$ the design matrix, $\beta \in \RR ^p$ the vector of regressors and $\lambda >0$ the smoothing parameter. \par Show that there exists $\lambda _{\max }$ such that the minimizer of~\eqref {eq:lasso} is $\mathbf {0}_p$ (a $p$-dimensional vector of zeros) for any $\lambda > \lambda _{\max }$.}||exercise-2=={For a univariate signal $\mathbf {x}\in \mathbb {R}^n$ with $n$ samples, the convolutional dictionary learning task amounts to solving the following optimization problem: \par \begin {equation} \min _{(\mathbf {d}_k)_k, (\mathbf {z}_k)_k \\ \norm {\mathbf {d}_k}_2^2\leq 1} \quad \norm {\mathbf {x} - \sum _{k=1}^K \mathbf {z}_k * \mathbf {d}_k }^2_2 \quad + \quad \lambda \sum _{k=1}^K \norm {\mathbf {z}_k}_1 \end {equation} \par where $\mathbf {d}_k\in \mathbb {R}^L$ are the $K$ dictionary atoms (patterns), $\mathbf {z}_k\in \mathbb {R}^{N-L+1}$ are activations signals, and $\lambda >0$ is the smoothing parameter. \par Show that \begin {itemize} \item for a fixed dictionary, the sparse coding problem is a lasso regression (explicit the response vector and the design matrix); \item for a fixed dictionary, there exists $\lambda _{\max }$ (which depends on the dictionary) such that the sparse codes are only 0 for any $\lambda > \lambda _{\max }$. \end {itemize}}||exercise-3=={In this question, let $X_n$ ($n=0,\dots ,N-1)$ be a Gaussian white noise. \par \begin {itemize} \item Calculate the associated autocovariance function and power spectrum. (By analogy with the light, this process is called ``white'' because of the particular form of its power spectrum.) \end {itemize} \par }||exercise-4=={A natural estimator for the autocorrelation function is the sample autocovariance \begin {equation} \hat {\gamma }(\tau ) := (1/N) \sum _{n=0}^{N-\tau -1} X_n X_{n+\tau } \end {equation} for $\tau =0,1,\dots ,N-1$ and $\hat {\gamma }(\tau ):=\hat {\gamma }(-\tau )$ for $\tau =-(N-1),\dots ,-1$. \begin {itemize} \item Show that $\hat {\gamma }(\tau )$ is a biased estimator of $\gamma (\tau )$ but asymptotically unbiased. What would be a simple way to de-bias this estimator? \end {itemize} \par }||exercise-5=={Define the discrete Fourier transform of the random process $\{X_n\}_n$ by \begin {equation} J(f) := (1/\sqrt {N})\sum _{n=0}^{N-1} X_n e^{-2\pi \iu f n/f_s} \end {equation} The \textit {periodogram} is the collection of values $|J(f_0)|^2$, $|J(f_1)|^2$, \dots , $|J(f_{N/2})|^2$ where $f_k = f_s k/N$. (They can be efficiently computed using the Fast Fourier Transform.) \begin {itemize} \item Write $|J(f_k)|^2$ as a function of the sample autocovariances. \item For a frequency $f$, define $f^{(N)}$ the closest Fourier frequency $f_k$ to $f$. Show that $|J(f^{(N)})|^2$ is an asymptotically unbiased estimator of $S(f)$ for $f>0$. \end {itemize}}||exercise-6=={\label {ex:wn-exp} In this question, let $X_n$ ($n=0,\dots ,N-1)$ be a Gaussian white noise with variance $\sigma ^2=1$ and set the sampling frequency to $f_s=1$ Hz \begin {itemize} \item For $N\in \{200, 500, 1000\}$, compute the \textit {sample autocovariances} ($\hat {\gamma }(\tau )$ vs $\tau $) for 100 simulations of $X$. Plot the average value as well as the average $\pm $, the standard deviation. What do you observe? \item For $N\in \{200, 500, 1000\}$, compute the \textit {periodogram} ($|J(f_k)|^2$ vs $f_k$) for 100 simulations of $X$. Plot the average value as well as the average $\pm $, the standard deviation. What do you observe? \end {itemize} Add your plots to Figure~\ref {fig:wn-exp}. \par \begin {figure} \centering \begin {minipage}[t]{0.3\textwidth } \centerline {\includegraphics [width=\textwidth ]{sample_autocovariance200.png}} \centerline {Autocovariance ($N=200$)} \end {minipage} \begin {minipage}[t]{0.3\textwidth } \centerline {\includegraphics [width=\textwidth ]{sample_autocovariance500.png}} \centerline {Autocovariance ($N=500$)} \end {minipage} \begin {minipage}[t]{0.3\textwidth } \centerline {\includegraphics [width=\textwidth ]{sample_autocovariance1000.png}} \centerline {Autocovariance ($N=1000$)} \end {minipage} \vskip 1em \begin {minipage}[t]{0.3\textwidth } \centerline {\includegraphics [width=\textwidth ]{periodogram200.png}} \centerline {Periodogram ($N=200$)} \end {minipage} \begin {minipage}[t]{0.3\textwidth } \centerline {\includegraphics [width=\textwidth ]{periodogram500.png}} \centerline {Periodogram ($N=500$)} \end {minipage} \begin {minipage}[t]{0.3\textwidth } \centerline {\includegraphics [width=\textwidth ]{periodogram1000.png}} \centerline {Periodogram ($N=1000$)} \end {minipage} \vskip 1em \caption {Autocovariances and periodograms of a Gaussian white noise (see Question~\ref {ex:wn-exp}).} \label {fig:wn-exp} \end {figure} \par }||exercise-7=={We want to show that the estimator $\hat {\gamma }(\tau )$ is consistent, \ie it converges in probability when the number $N$ of samples grows to $\infty $ to the true value ${\gamma }(\tau )$. In this question, assume that $X$ is a wide-sense stationary \textit {Gaussian} process. \begin {itemize} \item Show that for $\tau >0$ \begin {equation} \text {var}(\hat {\gamma }(\tau )) = (1/N) \sum _{n=-(N-\tau -1)}^{n=N-\tau -1} \left (1 - \frac {\tau + |n|}{N}\right ) \left [\gamma ^2(n) + \gamma (n-\tau )\gamma (n+\tau )\right ]. \end {equation} (Hint: if $\{Y_1, Y_2, Y_3, Y_4\}$ are four centered jointly Gaussian variables, then $\mathbb {E}[Y_1 Y_2 Y_3 Y_4] = \mathbb {E}[Y_1 Y_2]\mathbb {E}[Y_3 Y_4] + \mathbb {E}[Y_1 Y_3]\mathbb {E}[Y_2 Y_4] + \mathbb {E}[Y_1 Y_4]\mathbb {E}[Y_2 Y_3]$.) \item Conclude that $\hat {\gamma }(\tau )$ is consistent. \end {itemize}}||exercise-8=={Assume that $X$ is a Gaussian white noise (variance $\sigma ^2$) and let $A(f):=\sum _{n=0}^{N-1} X_n \cos (-2\pi f n/f_s$ and $B(f):=\sum _{n=0}^{N-1} X_n \sin (-2\pi f n/f_s$. Observe that $J(f) = (1/N) (A(f) + \iu B(f))$. \begin {itemize} \item Derive the mean and variance of $A(f)$ and $B(f)$ for $f=f_0, f_1,\dots , f_{N/2}$ where $f_k=f_s k/N$. \item What is the distribution of the periodogram values $|J(f_0)|^2$, $|J(f_1)|^2$, \dots , $|J(f_{N/2})|^2$. \item What is the variance of the $|J(f_k)|^2$? Conclude that the periodogram is not consistent. \item Explain the erratic behavior of the periodogram in Question~\ref {ex:wn-exp} by looking at the covariance between the $|J(f_k)|^2$. \end {itemize} \par }||exercise-9=={\label {q:barlett} As seen in the previous question, the problem with the periodogram is the fact that its variance does not decrease with the sample size. A simple procedure to obtain a consistent estimate is to divide the signal into $K$ sections of equal durations, compute a periodogram on each section, and average them. Provided the sections are independent, this has the effect of dividing the variance by $K$. This procedure is known as Bartlett's procedure. \begin {itemize} \item Rerun the experiment of Question~\ref {ex:wn-exp}, but replace the periodogram by Barlett's estimate (set $K=5$). What do you observe? \end {itemize} Add your plots to Figure~\ref {fig:barlett}.}||exercise-10=={Combine the DTW and a k-neighbors classifier to classify each step. Find the optimal number of neighbors with 5-fold cross-validation and report the optimal number of neighbors and the associated F-score. Comment briefly.}||exercise-11=={\label {q:class-errors} Display on Figure~\ref {fig:class-errors} a badly classified step from each class (healthy/non-healthy).}}
\XSIM{goal}{exercise}{points}{0}
\XSIM{totalgoal}{points}{0}
\XSIM{goal}{exercise}{bonus-points}{0}
\XSIM{totalgoal}{bonus-points}{0}
\XSIM{order}{1||2||3||4||5||6||7||8||9||10||11}
\XSIM{use}{}
\XSIM{use!}{}
\XSIM{used}{exercise-1=={true}||exercise-2=={true}||exercise-3=={true}||exercise-4=={true}||exercise-5=={true}||exercise-6=={true}||exercise-7=={true}||exercise-8=={true}||exercise-9=={true}||exercise-10=={true}||exercise-11=={true}}
\XSIM{print}{}
\XSIM{print!}{}
\XSIM{printed}{exercise-1=={true}||exercise-2=={true}||exercise-3=={true}||exercise-4=={true}||exercise-5=={true}||exercise-6=={true}||exercise-7=={true}||exercise-8=={true}||exercise-9=={true}||exercise-10=={true}||exercise-11=={true}}
\XSIM{total-number}{11}
\XSIM{exercise}{11}
\XSIM{types}{exercise}
\XSIM{idtypes}{1=={exercise}||2=={exercise}||3=={exercise}||4=={exercise}||5=={exercise}||6=={exercise}||7=={exercise}||8=={exercise}||9=={exercise}||10=={exercise}||11=={exercise}}
\XSIM{collections}{exercise-1=={all exercises}||exercise-2=={all exercises}||exercise-3=={all exercises}||exercise-4=={all exercises}||exercise-5=={all exercises}||exercise-6=={all exercises}||exercise-7=={all exercises}||exercise-8=={all exercises}||exercise-9=={all exercises}||exercise-10=={all exercises}||exercise-11=={all exercises}}
\XSIM{collection:all exercises}{exercise-1||exercise-2||exercise-3||exercise-4||exercise-5||exercise-6||exercise-7||exercise-8||exercise-9||exercise-10||exercise-11}
\setcounter{totalexerciseinall exercises}{11}
\XSIM{id}{exercise-1=={1}||exercise-2=={2}||exercise-3=={3}||exercise-4=={4}||exercise-5=={5}||exercise-6=={6}||exercise-7=={7}||exercise-8=={8}||exercise-9=={9}||exercise-10=={10}||exercise-11=={11}}
\XSIM{ID}{exercise-1=={1}||exercise-2=={2}||exercise-3=={3}||exercise-4=={4}||exercise-5=={5}||exercise-6=={6}||exercise-7=={7}||exercise-8=={8}||exercise-9=={9}||exercise-10=={10}||exercise-11=={11}}
\XSIM{counter}{exercise-1=={1}||exercise-2=={2}||exercise-3=={3}||exercise-4=={4}||exercise-5=={5}||exercise-6=={6}||exercise-7=={7}||exercise-8=={8}||exercise-9=={9}||exercise-10=={10}||exercise-11=={11}}
\XSIM{counter-value}{exercise-1=={1}||exercise-2=={2}||exercise-3=={3}||exercise-4=={4}||exercise-5=={5}||exercise-6=={6}||exercise-7=={7}||exercise-8=={8}||exercise-9=={9}||exercise-10=={10}||exercise-11=={11}}
\XSIM{solution}{}
\XSIM{section-value}{exercise-1=={2}||exercise-2=={2}||exercise-3=={3}||exercise-4=={3}||exercise-5=={3}||exercise-6=={3}||exercise-7=={3}||exercise-8=={3}||exercise-9=={3}||exercise-10=={4}||exercise-11=={4}}
\XSIM{section}{exercise-1=={2}||exercise-2=={2}||exercise-3=={3}||exercise-4=={3}||exercise-5=={3}||exercise-6=={3}||exercise-7=={3}||exercise-8=={3}||exercise-9=={3}||exercise-10=={4}||exercise-11=={4}}
\XSIM{sectioning}{exercise-1=={{0}{2}{0}{0}{0}}||exercise-2=={{0}{2}{0}{0}{0}}||exercise-3=={{0}{3}{0}{0}{0}}||exercise-4=={{0}{3}{0}{0}{0}}||exercise-5=={{0}{3}{0}{0}{0}}||exercise-6=={{0}{3}{0}{0}{0}}||exercise-7=={{0}{3}{0}{0}{0}}||exercise-8=={{0}{3}{0}{0}{0}}||exercise-9=={{0}{3}{0}{0}{0}}||exercise-10=={{0}{4}{2}{0}{0}}||exercise-11=={{0}{4}{2}{0}{0}}}
\XSIM{subtitle}{}
\XSIM{points}{}
\XSIM{bonus-points}{}
\XSIM{page-value}{exercise-1=={1}||exercise-2=={2}||exercise-3=={2}||exercise-4=={3}||exercise-5=={3}||exercise-6=={4}||exercise-7=={5}||exercise-8=={6}||exercise-9=={6}||exercise-10=={7}||exercise-11=={9}}
\XSIM{page}{exercise-1=={1}||exercise-2=={2}||exercise-3=={2}||exercise-4=={3}||exercise-5=={3}||exercise-6=={4}||exercise-7=={5}||exercise-8=={6}||exercise-9=={6}||exercise-10=={7}||exercise-11=={9}}
\XSIM{tags}{}
\XSIM{topics}{}
\XSIM{userpoints}{}
\XSIM{bodypoints}{}
\XSIM{userbonus-points}{}
\XSIM{bodybonus-points}{}
